{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1VOrVEgoRQFZbyI7stQy5Vb0J0rjFLdW8",
      "authorship_tag": "ABX9TyN8NiMRS240VgvAbEo5FBbO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lakshika-wijesundara/BlazorApp/blob/master/Ai_Project_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Paths to files\n",
        "dictionary_path = '/content/drive/MyDrive/AI project/extended_sinhala_dictionary.txt'\n",
        "sentences_path = '/content/drive/MyDrive/AI project/sentences.txt'\n",
        "\n",
        "# Load dictionary\n",
        "def load_dictionary(dictionary_path):\n",
        "    with open(dictionary_path, 'r', encoding='utf-8') as file:\n",
        "        return set(word.strip() for word in file)\n",
        "\n",
        "# Update dictionary with words from sentences file\n",
        "def update_dictionary(dictionary_path, sentences_path):\n",
        "    dictionary = load_dictionary(dictionary_path)\n",
        "\n",
        "    # Extract words from sentences\n",
        "    with open(sentences_path, 'r', encoding='utf-8') as file:\n",
        "        sentences = file.readlines()\n",
        "\n",
        "    for sentence in sentences:\n",
        "        words = sentence.strip().split()\n",
        "        dictionary.update(words)\n",
        "\n",
        "    # Save updated dictionary\n",
        "    with open(dictionary_path, 'w', encoding='utf-8') as file:\n",
        "        file.write('\\n'.join(sorted(dictionary)))\n",
        "\n",
        "# Initialize dictionary\n",
        "update_dictionary(dictionary_path, sentences_path)"
      ],
      "metadata": {
        "id": "YxiBytt8yngg"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Paths to files\n",
        "dictionary_path = '/content/drive/MyDrive/AI project/extended_sinhala_dictionary.txt'\n",
        "sentences_path = '/content/drive/MyDrive/AI project/sentences.txt'\n",
        "\n",
        "# Load dictionary\n",
        "def load_dictionary(dictionary_path):\n",
        "    with open(dictionary_path, 'r', encoding='utf-8') as file:\n",
        "        return set(word.strip() for word in file)\n",
        "\n",
        "# Update dictionary with words from sentences file\n",
        "def update_dictionary(dictionary_path, sentences_path):\n",
        "    dictionary = load_dictionary(dictionary_path)\n",
        "\n",
        "    # Extract words from sentences\n",
        "    with open(sentences_path, 'r', encoding='utf-8') as file:\n",
        "        sentences = file.readlines()\n",
        "\n",
        "    for sentence in sentences:\n",
        "        words = sentence.strip().split()\n",
        "        dictionary.update(words)\n",
        "\n",
        "    # Save updated dictionary\n",
        "    with open(dictionary_path, 'w', encoding='utf-8') as file:\n",
        "        file.write('\\n'.join(sorted(dictionary)))\n",
        "\n",
        "# Initialize dictionary\n",
        "update_dictionary(dictionary_path, sentences_path)\n",
        "\n",
        "import re\n",
        "from difflib import get_close_matches\n",
        "\n",
        "# Spell checker\n",
        "def spell_checker(text, dictionary):\n",
        "    words = text.split()\n",
        "    errors = {}\n",
        "    for word in words:\n",
        "        if word not in dictionary:\n",
        "            suggestions = get_close_matches(word, dictionary, n=3)\n",
        "            errors[word] = suggestions\n",
        "    return errors\n",
        "\n",
        "# Grammar checker (basic example: contextual grammar issues)\n",
        "def grammar_checker(text):\n",
        "    grammar_errors = []\n",
        "    # Example: incorrect use of duplicate words\n",
        "    duplicate_pattern = re.compile(r'\\b(\\w+) \\1\\b')\n",
        "    for match in duplicate_pattern.finditer(text):\n",
        "        grammar_errors.append((match.group(), match.start(), match.end()))\n",
        "    return grammar_errors\n",
        "\n",
        "# Highlight errors\n",
        "def highlight_errors(text, spell_errors, grammar_errors):\n",
        "    for word, suggestions in spell_errors.items():\n",
        "        text = text.replace(word, f'<span style=\"color:red;\">{word}</span>')\n",
        "    for error, start, end in grammar_errors:\n",
        "        text = text[:start] + f'<span style=\"color:red;\">{error}</span>' + text[end:]\n",
        "    return text\n",
        "\n",
        "# Example usage\n",
        "dictionary = load_dictionary(dictionary_path)\n",
        "input_text = \"අපි හිරු උදාව බලා සිටිමු. අපි කාර්යබහුල වීදිය හරහා ගමන් කළෙමු. ළමයි දවසට සත්තු වත්තට ගියහ. මම මගේ මිතුරන් සමඟ පැසිපන්දු ක්‍රීඩා කලෙමි. ඇය නවකතාවේ පරිච්ඡේදයක් කියෙව්වාය. මම මනස සැහැල්ලු කර ගැනීමට යෝග ක්‍රියාකාරකම් කලෙමි. ඔවුහු සැමරුමට එක් වූහ.\"\n",
        "spell_errors = spell_checker(input_text, dictionary)\n",
        "grammar_errors = grammar_checker(input_text)\n",
        "corrected_text = highlight_errors(input_text, spell_errors, grammar_errors)\n",
        "\n",
        "print(\"Spell Errors:\", spell_errors)\n",
        "print(\"Grammar Errors:\", grammar_errors)\n",
        "print(\"Corrected Text:\", corrected_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8C3IuzXYylXk",
        "outputId": "7c76e6be-ec7a-4167-d6f0-1cd2b7154871"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spell Errors: {}\n",
            "Grammar Errors: []\n",
            "Corrected Text: අපි හිරු උදාව බලා සිටිමු. අපි කාර්යබහුල වීදිය හරහා ගමන් කළෙමු. ළමයි දවසට සත්තු වත්තට ගියහ. මම මගේ මිතුරන් සමඟ පැසිපන්දු ක්‍රීඩා කලෙමි. ඇය නවකතාවේ පරිච්ඡේදයක් කියෙව්වාය. මම මනස සැහැල්ලු කර ගැනීමට යෝග ක්‍රියාකාරකම් කලෙමි. ඔවුහු සැමරුමට එක් වූහ.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mSGklqm2qQC",
        "outputId": "91822e80-66bd-40d3-ee2b-96b562a7a395"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "git is already the newest version (1:2.34.1-1ubuntu1.11).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/lakshika-wijesundara/Sinhala-Spell-and-Grammer-Checker.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ncRDNh62tu8",
        "outputId": "1c100fd3-b19d-40c4-8eb7-9627374aca7f"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Sinhala-Spell-and-Grammer-Checker'...\n",
            "remote: Enumerating objects: 49, done.\u001b[K\n",
            "remote: Counting objects: 100% (49/49), done.\u001b[K\n",
            "remote: Compressing objects: 100% (47/47), done.\u001b[K\n",
            "remote: Total 49 (delta 5), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (49/49), 230.67 KiB | 2.51 MiB/s, done.\n",
            "Resolving deltas: 100% (5/5), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/<Sinhala-Spell-and-Grammer-Checker>.py'\n",
        "with open(file_path, 'w') as file:\n",
        "    file.write('''<Ai Project.ipynb>''')\n"
      ],
      "metadata": {
        "id": "xgVeA_zC58T_"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/Sinhala-Spell-and-Grammer-Checker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTbKWz4Y8DZy",
        "outputId": "4a873938-c174-4910-d493-a63ca89fd83e"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Sinhala-Spell-and-Grammer-Checker\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/<Ai Project>.ipynb/content/<Sinhala-Spell-and-Grammer-Checker>/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6MEAaYS7XaD",
        "outputId": "06159df3-bbc4-40e0-e4a8-e5fbe1dec05e"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: Ai: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git add <Ai Project>.ipynb\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hu-MfRaE9TkZ",
        "outputId": "d1b277df-daab-4d46-834c-de38c5a064b0"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: Ai: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5NrxFlUD9p4V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}